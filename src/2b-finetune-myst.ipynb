{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af974f5a-2f03-45a4-8bb9-317bf85a9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    pipeline,\n",
    ")\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import re\n",
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222def0e-1b0a-4d70-b44a-e9570baa6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "torch_dtype = torch.float32\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Configure generation\n",
    "model.generation_config.language = \"english\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5299154-39dd-4d63-8241-5307658976cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b8de0249ab42549e263f410a3b6883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription', 'speaker_id', 'date', 'time', 'session_type', 'version', 'split'],\n",
       "        num_rows: 76924\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'transcription', 'speaker_id', 'date', 'time', 'session_type', 'version', 'split'],\n",
       "        num_rows: 12238\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'transcription', 'speaker_id', 'date', 'time', 'session_type', 'version', 'split'],\n",
       "        num_rows: 13169\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myst = load_from_disk(\n",
    "    \"/home/jovyan/active-projects/tla-asr-finetune/data/myst_dataset.ds\"\n",
    ")\n",
    "myst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4f238b-1022-4774-aa58-918adfcaafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "\n",
    "def normalize_transcript(text):\n",
    "    # The original transcript has annotations, for example a pause is <pau>\n",
    "    # Remove tags in angle brackets\n",
    "    text = re.sub(r\"<[^>]*>\", \"\", text)\n",
    "\n",
    "    # These are \"false starts\" in the original transcript, for example th*\n",
    "    # These are ignored by ASR\n",
    "    # Remove words that end with asterisks (e.g., th*)\n",
    "    text = re.sub(r\"\\S*\\*\", \"\", text)\n",
    "\n",
    "    # Remove all punctuation\n",
    "    normalized_text = normalizer(text)\n",
    "\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c95c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36544\n"
     ]
    }
   ],
   "source": [
    "def filter_short_long_samples(batch):\n",
    "    audio_len = batch[\"audio\"][\"array\"].shape[0] / batch[\"audio\"][\"sampling_rate\"]\n",
    "    return 5 <= audio_len <= 30\n",
    "\n",
    "\n",
    "def filter_empty_transcripts(batch):\n",
    "    return len(batch[\"transcription\"].strip()) > 0\n",
    "\n",
    "\n",
    "myst_filtered = myst.filter(filter_short_long_samples).filter(filter_empty_transcripts)\n",
    "\n",
    "print(len(myst_filtered[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860b1c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_features', 'labels'],\n",
       "    num_rows: 36544\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # Load audio\n",
    "    audio = batch[\"audio\"]\n",
    "    sampling_rate = batch[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "    inputs = processor.feature_extractor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",  # This ensures padding to max length in batch\n",
    "        max_length=30 * sampling_rate,  # 30 seconds at 16kHz\n",
    "        truncation=True,  # Truncate if longer than max_length\n",
    "    )\n",
    "\n",
    "    # Reprocess the filtered audio\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "\n",
    "    # Normalize and encode target text\n",
    "    normalized_text = normalize_transcript(batch[\"transcription\"])\n",
    "    batch[\"labels\"] = processor.tokenizer(normalized_text, padding=True).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "myst_processed = myst_filtered.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=myst_filtered.column_names[\"train\"],\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "myst_processed[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6550b7-a019-4393-9a0e-8352d79d394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_wer(ref: list[str], pred: list[str]):\n",
    "    # Normalize both predictions and references\n",
    "    pred_normalized = [normalize_transcript(text) for text in pred]\n",
    "    label_normalized = [normalize_transcript(text) for text in ref]\n",
    "\n",
    "    total_errors = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for pred_text, ref_text in zip(pred_normalized, label_normalized):\n",
    "        ref_words = ref_text.split()\n",
    "\n",
    "        # Compute WER for this sample\n",
    "        if len(ref_words) > 0:\n",
    "            sample_wer = jiwer.wer(ref_text, pred_text)\n",
    "        else:\n",
    "            sample_wer = 0\n",
    "\n",
    "        # Accumulate weighted errors\n",
    "        sample_errors = sample_wer * len(ref_words)\n",
    "        total_errors += sample_errors\n",
    "        total_words += len(ref_words)\n",
    "\n",
    "    weighted_wer = total_errors / total_words if total_words > 0 else 0.0\n",
    "\n",
    "    return {\"wer\": weighted_wer}\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with pad token\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    return weighted_wer(label_str, pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a34aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription', 'speaker_id', 'date', 'time', 'session_type', 'version', 'split'],\n",
       "        num_rows: 76924\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'transcription', 'speaker_id', 'date', 'time', 'session_type', 'version', 'split'],\n",
       "        num_rows: 12238\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'transcription', 'speaker_id', 'date', 'time', 'session_type', 'version', 'split'],\n",
       "        num_rows: 13169\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c2409ed-4d8a-41f7-9eee-664c3708c6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/jovyan/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     wer_score \u001b[38;5;241m=\u001b[39m weighted_wer(y_true, y_pred)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(wer_score)\n\u001b[0;32m---> 19\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mget_baseline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m get_wer(myst[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m\"\u001b[39m], preds)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 0.19560642190320807\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mget_baseline\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_baseline\u001b[39m(ds):\n\u001b[1;32m      2\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomatic-speech-recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m         chunk_length_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m     10\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py:283\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    224\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    226\u001b[0m ):\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/pipelines/base.py:1349\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1346\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1347\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1348\u001b[0m     )\n\u001b[0;32m-> 1349\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py:607\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.postprocess\u001b[0;34m(self, model_outputs, decoder_kwargs, return_timestamps, return_language)\u001b[0m\n\u001b[1;32m    604\u001b[0m             stride_right \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m sampling_rate\n\u001b[1;32m    605\u001b[0m             output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk_len, stride_left, stride_right\n\u001b[0;32m--> 607\u001b[0m     text, optional \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode_asr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    614\u001b[0m     items \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(final_items, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/tokenization_whisper.py:855\u001b[0m, in \u001b[0;36mWhisperTokenizer._decode_asr\u001b[0;34m(self, model_outputs, return_timestamps, return_language, time_precision)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode_asr\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_outputs, \u001b[38;5;241m*\u001b[39m, return_timestamps, return_language, time_precision):\n\u001b[0;32m--> 855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_decode_asr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/tokenization_whisper.py:1109\u001b[0m, in \u001b[0;36m_decode_asr\u001b[0;34m(tokenizer, model_outputs, return_timestamps, return_language, time_precision)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;66;03m# Happens when we don't use timestamps\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m resolved_tokens, resolved_token_timestamps \u001b[38;5;241m=\u001b[39m _find_longest_common_sequence(\n\u001b[1;32m   1107\u001b[0m     previous_tokens, previous_token_timestamps\n\u001b[1;32m   1108\u001b[0m )\n\u001b[0;32m-> 1109\u001b[0m resolved_text \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m resolved_text\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_timestamps \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/tokenization_whisper.py:728\u001b[0m, in \u001b[0;36mWhisperTokenizer.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, output_offsets, time_precision, decode_with_timestamps, normalize, basic_normalize, remove_diacritics, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    723\u001b[0m filtered_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_token_ids(\n\u001b[1;32m    724\u001b[0m     token_ids,\n\u001b[1;32m    725\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m    726\u001b[0m )\n\u001b[0;32m--> 728\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbasic_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasic_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_diacritics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_diacritics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decode_with_timestamps:\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;66;03m# legacy method to decode timestamps when not included in the tokenizer vocabulary\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_with_timestamps(\n\u001b[1;32m    740\u001b[0m         filtered_ids, time_precision\u001b[38;5;241m=\u001b[39mtime_precision, skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens\n\u001b[1;32m    741\u001b[0m     )\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3860\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3857\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3858\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/tokenization_whisper.py:771\u001b[0m, in \u001b[0;36mWhisperTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, normalize, basic_normalize, remove_diacritics, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madded_tokens_encoder\u001b[49m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_sub_text:\n\u001b[1;32m    773\u001b[0m         sub_texts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_string(current_sub_text))\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/tokenization_utils.py:463\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.added_tokens_encoder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madded_tokens_encoder\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    Returns the sorted mapping from string to index. The added tokens encoder is cached for performance\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m    optimisation in `self._added_tokens_encoder` for the slow tokenizers.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k\u001b[38;5;241m.\u001b[39mcontent: v \u001b[38;5;28;01mfor\u001b[39;00m v, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m0\u001b[39m])}\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/tokenization_utils.py:463\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.added_tokens_encoder.<locals>.<lambda>\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madded_tokens_encoder\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    Returns the sorted mapping from string to index. The added tokens encoder is cached for performance\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m    optimisation in `self._added_tokens_encoder` for the slow tokenizers.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k\u001b[38;5;241m.\u001b[39mcontent: v \u001b[38;5;28;01mfor\u001b[39;00m v, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m0\u001b[39m])}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_baseline(ds):\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        batch_size=16,\n",
    "        device=device,\n",
    "        chunk_length_s=30,\n",
    "    )\n",
    "    results = pipe(ds[\"test\"][\"audio\"])\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_wer(y_true, preds):\n",
    "    y_pred = [d[\"text\"] for d in preds]\n",
    "    wer_score = weighted_wer(y_true, y_pred)\n",
    "    return wer_score\n",
    "\n",
    "\n",
    "preds = get_baseline(myst)\n",
    "get_wer(myst[\"test\"][\"transcription\"], preds)\n",
    "# 0.19560642190320807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789eace5-92d6-4665-b8af-a9b43ce5f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_by_module():\n",
    "    grades_3_5 = [\"ME\", \"MS\", \"VB\", \"WA\"]\n",
    "    grades_4_5 = [\"EE\", \"MX\", \"SMP\", \"SRL\", \"LS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c89eff1e-140d-43ad-a4b9-c702a2bc7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06954565-b7cc-4bf3-8a47-3d3b5543e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../bin/whisper-myst\",\n",
    "    per_device_train_batch_size=8,  # About 30 GiB of VRAM with 2x gradient accumulation\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-6,\n",
    "    warmup_steps=100,\n",
    "    max_steps=1_000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,  # About 30GiB of VRAM\n",
    "    predict_with_generate=True,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    logging_steps=50,\n",
    "    report_to=[],  # Disable logging for simplicity\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=myst_processed[\"train\"],\n",
    "    eval_dataset=myst_processed[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1f4e339-3c2f-4a0f-b6b8-ad6ec32252ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 10:01:30, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.587100</td>\n",
       "      <td>0.549029</td>\n",
       "      <td>0.178696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.345500</td>\n",
       "      <td>0.414291</td>\n",
       "      <td>0.150277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>0.401005</td>\n",
       "      <td>0.137361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.137282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.391985</td>\n",
       "      <td>0.136113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda_envs/hf/lib/python3.11/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.47948250007629395, metrics={'train_runtime': 36098.4593, 'train_samples_per_second': 0.443, 'train_steps_per_second': 0.028, 'total_flos': 5.435997290496e+19, 'train_loss': 0.47948250007629395, 'epoch': 0.43782837127845886})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# 0.136113 with learning_rate 1e-6 on MyST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4948ba0-f8a5-40f2-9bfe-b701f4ab9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../bin/myst1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "984a9576-27fe-442d-8f26-f9b32e8d1001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating final model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='396' max='396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [396/396 56:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final WER: 0.1361\n"
     ]
    }
   ],
   "source": [
    "# Evaluate final model\n",
    "print(\"Evaluating final model...\")\n",
    "final_results = trainer.evaluate()\n",
    "print(f\"Final WER: {final_results['eval_wer']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2f70e-6e49-40e8-bfe2-a5cdf315e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few samples\n",
    "print(\"\\nTesting on sample predictions:\")\n",
    "test_samples = cslu_processed[\"test\"].select(range(3))\n",
    "predictions = trainer.predict(test_samples)\n",
    "\n",
    "pred_ids = predictions.predictions\n",
    "pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "for i, (pred, label) in enumerate(zip(pred_str, test_samples[\"labels\"])):\n",
    "    label_str = processor.tokenizer.decode(label, skip_special_tokens=True)\n",
    "    print(f\"\\nSample {i + 1}:\")\n",
    "    print(f\"Prediction: {normalize_transcript(pred)}\")\n",
    "    print(f\"Reference:  {normalize_transcript(label_str)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
