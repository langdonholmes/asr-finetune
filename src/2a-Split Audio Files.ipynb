{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6b345e5-c1dc-41a6-b1e4-7460c500a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "cslu = load_from_disk(\"../data/cslu_kids.ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94af097c-de04-4307-b754-80da5b11471f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True,\n",
    "\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=1,  # batch size for inference - set based on your device\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64df61ac-ffbe-47ff-9f6d-3b22b7d8b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_by_timestamps(audio_array, sampling_rate, chunks):\n",
    "    \"\"\"\n",
    "    Split audio array into segments based on timestamp chunks.\n",
    "    \n",
    "    Args:\n",
    "        audio_array: numpy array of audio samples\n",
    "        sampling_rate: sampling rate of audio\n",
    "        chunks: list of dicts with 'text' and 'timestamp' keys\n",
    "    \n",
    "    Returns:\n",
    "        list of (audio_segment, text, start_time, end_time) tuples\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        text = chunk['text'].strip()\n",
    "        timestamp = chunk['timestamp']\n",
    "        \n",
    "        if timestamp[0] is None or timestamp[1] is None:\n",
    "            # Skip chunks with invalid timestamps\n",
    "            continue\n",
    "            \n",
    "        start_time, end_time = timestamp\n",
    "        \n",
    "        # Convert time to sample indices\n",
    "        start_sample = int(start_time * sampling_rate)\n",
    "        end_sample = int(end_time * sampling_rate)\n",
    "        \n",
    "        # Extract audio segment\n",
    "        audio_segment = audio_array[start_sample:end_sample]\n",
    "        \n",
    "        # Skip very short segments (< 0.1 seconds)\n",
    "        if len(audio_segment) < sampling_rate * 0.1:\n",
    "            continue\n",
    "            \n",
    "        segments.append({\n",
    "            'audio': audio_segment,\n",
    "            'text': text,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'duration': end_time - start_time\n",
    "        })\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def extract_words_from_transcript(transcript):\n",
    "    \"\"\"\n",
    "    Extract words from the original CSLU transcript, removing annotations.\n",
    "    \"\"\"\n",
    "    # Remove annotations in angle brackets\n",
    "    text = re.sub(r'<[^>]*>', '', transcript)\n",
    "    \n",
    "    # Remove false starts (words ending with *)\n",
    "    text = re.sub(r'\\S*\\*', '', text)\n",
    "    \n",
    "    # Split into words and clean up\n",
    "    words = text.split()\n",
    "    words = [word.strip() for word in words if word.strip()]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def align_segments_with_transcript(segments, original_transcript):\n",
    "    \"\"\"\n",
    "    Align Whisper segments with the original transcript to create training pairs.\n",
    "    \n",
    "    Args:\n",
    "        segments: list of audio segments from split_audio_by_timestamps\n",
    "        original_transcript: original CSLU transcript string\n",
    "    \n",
    "    Returns:\n",
    "        list of aligned (audio_segment, gold_text) pairs\n",
    "    \"\"\"\n",
    "    # Extract words from original transcript\n",
    "    gold_words = extract_words_from_transcript(original_transcript)\n",
    "    \n",
    "    # Extract words from Whisper segments\n",
    "    whisper_words = []\n",
    "    for segment in segments:\n",
    "        words = segment['text'].split()\n",
    "        whisper_words.extend(words)\n",
    "    \n",
    "    # Normalize for comparison (lowercase, remove punctuation)\n",
    "    def normalize_word(word):\n",
    "        return re.sub(r'[^\\w]', '', word.lower())\n",
    "    \n",
    "    gold_normalized = [normalize_word(w) for w in gold_words]\n",
    "    whisper_normalized = [normalize_word(w) for w in whisper_words]\n",
    "    \n",
    "    # Use sequence matching to align\n",
    "    matcher = SequenceMatcher(None, whisper_normalized, gold_normalized)\n",
    "    matches = matcher.get_matching_blocks()\n",
    "    \n",
    "    aligned_pairs = []\n",
    "    segment_idx = 0\n",
    "    word_idx_in_segment = 0\n",
    "    \n",
    "    for match in matches:\n",
    "        whisper_start, gold_start, length = match.a, match.b, match.size\n",
    "        \n",
    "        if length == 0:\n",
    "            continue\n",
    "        \n",
    "        # Find which segments these words belong to\n",
    "        current_whisper_word = 0\n",
    "        \n",
    "        for seg_i, segment in enumerate(segments):\n",
    "            seg_words = segment['text'].split()\n",
    "            \n",
    "            if current_whisper_word <= whisper_start < current_whisper_word + len(seg_words):\n",
    "                # This segment contains part of the match\n",
    "                \n",
    "                # Calculate which words in this segment are part of the match\n",
    "                seg_match_start = max(0, whisper_start - current_whisper_word)\n",
    "                seg_match_end = min(len(seg_words), whisper_start + length - current_whisper_word)\n",
    "                \n",
    "                if seg_match_end > seg_match_start:\n",
    "                    # Get corresponding gold words\n",
    "                    gold_match_start = gold_start + (seg_match_start - (whisper_start - current_whisper_word))\n",
    "                    gold_match_end = gold_match_start + (seg_match_end - seg_match_start)\n",
    "                    \n",
    "                    # Ensure we don't go out of bounds\n",
    "                    gold_match_end = min(gold_match_end, len(gold_words))\n",
    "                    \n",
    "                    if gold_match_start < len(gold_words) and gold_match_end > gold_match_start:\n",
    "                        gold_text = ' '.join(gold_words[gold_match_start:gold_match_end])\n",
    "                        \n",
    "                        aligned_pairs.append({\n",
    "                            'audio': segment['audio'],\n",
    "                            'whisper_text': ' '.join(seg_words[seg_match_start:seg_match_end]),\n",
    "                            'gold_text': gold_text,\n",
    "                            'start_time': segment['start_time'],\n",
    "                            'end_time': segment['end_time'],\n",
    "                            'duration': segment['duration']\n",
    "                        })\n",
    "            \n",
    "            current_whisper_word += len(seg_words)\n",
    "            \n",
    "            if current_whisper_word > whisper_start + length:\n",
    "                break\n",
    "    \n",
    "    return aligned_pairs\n",
    "\n",
    "def create_training_dataset_from_aligned_pairs(aligned_pairs, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    Create a HuggingFace dataset from aligned audio-text pairs.\n",
    "    \"\"\"\n",
    "    dataset_dict = {\n",
    "        'audio': [],\n",
    "        'sentence': [],\n",
    "        'whisper_text': [],\n",
    "        'duration': []\n",
    "    }\n",
    "    \n",
    "    for pair in aligned_pairs:\n",
    "        # Create audio dict in HuggingFace format\n",
    "        audio_dict = {\n",
    "            'array': pair['audio'],\n",
    "            'sampling_rate': sampling_rate\n",
    "        }\n",
    "        \n",
    "        dataset_dict['audio'].append(audio_dict)\n",
    "        dataset_dict['sentence'].append(pair['gold_text'])\n",
    "        dataset_dict['whisper_text'].append(pair['whisper_text'])\n",
    "        dataset_dict['duration'].append(pair['duration'])\n",
    "    \n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "def process_cslu_sample_with_timestamps(sample, pipe):\n",
    "    \"\"\"\n",
    "    Process a single CSLU sample to create split training data.\n",
    "    \n",
    "    Args:\n",
    "        sample: single sample from CSLU dataset\n",
    "        pipe: Whisper pipeline with return_timestamps=True capability\n",
    "    \n",
    "    Returns:\n",
    "        Dataset with split audio segments and aligned transcripts\n",
    "    \"\"\"\n",
    "    # Get timestamped output from Whisper\n",
    "    audio = sample['audio']\n",
    "    chunks = pipe(audio, return_timestamps=\"word\")[\"chunks\"]\n",
    "    \n",
    "    # Split audio based on timestamps\n",
    "    segments = split_audio_by_timestamps(\n",
    "        audio['array'], \n",
    "        audio['sampling_rate'], \n",
    "        chunks\n",
    "    )\n",
    "    \n",
    "    # Align with original transcript\n",
    "    aligned_pairs = align_segments_with_transcript(\n",
    "        segments, \n",
    "        sample['sentence']\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    if aligned_pairs:\n",
    "        return create_training_dataset_from_aligned_pairs(\n",
    "            aligned_pairs, \n",
    "            audio['sampling_rate']\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def create_split_dataset_from_cslu(cslu_dataset, pipe, max_samples=None):\n",
    "    \"\"\"\n",
    "    Process entire CSLU dataset to create split training data.\n",
    "    \n",
    "    Args:\n",
    "        cslu_dataset: CSLU dataset\n",
    "        pipe: Whisper pipeline\n",
    "        max_samples: maximum number of samples to process (for testing)\n",
    "    \n",
    "    Returns:\n",
    "        Combined dataset with all split segments\n",
    "    \"\"\"\n",
    "    all_datasets = []\n",
    "    \n",
    "    # Process each sample\n",
    "    num_samples = len(cslu_dataset) if max_samples is None else min(max_samples, len(cslu_dataset))\n",
    "    \n",
    "    print(f\"Processing {num_samples} samples...\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing sample {i}/{num_samples}\")\n",
    "        \n",
    "        try:\n",
    "            sample = cslu_dataset[i]\n",
    "            split_dataset = process_cslu_sample_with_timestamps(sample, pipe)\n",
    "            \n",
    "            if split_dataset is not None and len(split_dataset) > 0:\n",
    "                all_datasets.append(split_dataset)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if all_datasets:\n",
    "        # Concatenate all datasets\n",
    "        from datasets import concatenate_datasets\n",
    "        combined_dataset = concatenate_datasets(all_datasets)\n",
    "        \n",
    "        print(f\"Created {len(combined_dataset)} training segments from {num_samples} original samples\")\n",
    "        print(f\"Average segments per sample: {len(combined_dataset) / num_samples:.2f}\")\n",
    "        \n",
    "        return combined_dataset\n",
    "    else:\n",
    "        print(\"No valid segments created!\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "def create_split_cslu_dataset():\n",
    "    \"\"\"\n",
    "    Main function to create the split CSLU dataset.\n",
    "    \"\"\"\n",
    "    print(\"Creating split CSLU dataset...\")\n",
    "    \n",
    "    # Create split dataset (start with a small subset for testing)\n",
    "    split_dataset = create_split_dataset_from_cslu(\n",
    "        cslu, \n",
    "        pipe, \n",
    "        max_samples=50  # Start small for testing\n",
    "    )\n",
    "    \n",
    "    if split_dataset is not None:\n",
    "        # Filter out very short segments (< 1 second)\n",
    "        split_dataset = split_dataset.filter(lambda x: x['duration'] > 1.0)\n",
    "        \n",
    "        # Filter out very long segments (> 10 seconds) \n",
    "        split_dataset = split_dataset.filter(lambda x: x['duration'] < 10.0)\n",
    "        \n",
    "        print(f\"Final dataset size after filtering: {len(split_dataset)}\")\n",
    "        \n",
    "        # Show some examples\n",
    "        print(\"\\nExample segments:\")\n",
    "        for i in range(min(5, len(split_dataset))):\n",
    "            sample = split_dataset[i]\n",
    "            print(f\"Segment {i+1}:\")\n",
    "            print(f\"  Duration: {sample['duration']:.2f}s\")\n",
    "            print(f\"  Whisper: {sample['whisper_text']}\")\n",
    "            print(f\"  Gold: {sample['sentence']}\")\n",
    "            print()\n",
    "        \n",
    "        return split_dataset\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1c65f7-8054-4469-91c7-b704f5eab969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating split CSLU dataset...\n",
      "Processing 50 samples...\n",
      "Processing sample 0/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "WhisperModel is using WhisperSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sample 0: 'array'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the splitting process\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m split_cslu_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_split_cslu_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 260\u001b[0m, in \u001b[0;36mcreate_split_cslu_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating split CSLU dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Create split dataset (start with a small subset for testing)\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m split_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_split_dataset_from_cslu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcslu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start small for testing\u001b[39;49;00m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# Filter out very short segments (< 1 second)\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     split_dataset \u001b[38;5;241m=\u001b[39m split_dataset\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 230\u001b[0m, in \u001b[0;36mcreate_split_dataset_from_cslu\u001b[0;34m(cslu_dataset, pipe, max_samples)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     sample \u001b[38;5;241m=\u001b[39m cslu_dataset[i]\n\u001b[0;32m--> 230\u001b[0m     split_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_cslu_sample_with_timestamps\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m split_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(split_dataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    233\u001b[0m         all_datasets\u001b[38;5;241m.\u001b[39mappend(split_dataset)\n",
      "Cell \u001b[0;32mIn[6], line 181\u001b[0m, in \u001b[0;36mprocess_cslu_sample_with_timestamps\u001b[0;34m(sample, pipe)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Get timestamped output from Whisper\u001b[39;00m\n\u001b[1;32m    180\u001b[0m audio \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 181\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Split audio based on timestamps\u001b[39;00m\n\u001b[1;32m    184\u001b[0m segments \u001b[38;5;241m=\u001b[39m split_audio_by_timestamps(\n\u001b[1;32m    185\u001b[0m     audio[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    186\u001b[0m     audio[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    187\u001b[0m     chunks\n\u001b[1;32m    188\u001b[0m )\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py:283\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    224\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    226\u001b[0m ):\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/pipelines/base.py:1360\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1274\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1275\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py:521\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline._forward\u001b[0;34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    519\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 521\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# whisper longform generation stores timestamps in \"segments\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_timestamps \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq_whisper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:774\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m             proc\u001b[38;5;241m.\u001b[39mset_begin_index(decoder_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[1;32m    768\u001b[0m (\n\u001b[1;32m    769\u001b[0m     seek_sequences,\n\u001b[1;32m    770\u001b[0m     seek_outputs,\n\u001b[1;32m    771\u001b[0m     should_skip,\n\u001b[1;32m    772\u001b[0m     do_condition_on_prev_tokens,\n\u001b[1;32m    773\u001b[0m     model_output_type,\n\u001b[0;32m--> 774\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:965\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate_with_fallback\u001b[0;34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m model_output_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n\u001b[0;32m--> 965\u001b[0m seek_sequences, seek_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocess_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseek_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_bsz \u001b[38;5;241m<\u001b[39m batch_size:\n\u001b[1;32m    974\u001b[0m     seek_sequences \u001b[38;5;241m=\u001b[39m seek_sequences[:cur_bsz]\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:1109\u001b[0m, in \u001b[0;36mWhisperGenerationMixin._postprocess_outputs\u001b[0;34m(self, seek_outputs, decoder_input_ids, return_token_timestamps, generation_config, is_shortform)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values[batch_idx]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m   1108\u001b[0m sequence_tokens \u001b[38;5;241m=\u001b[39m seek_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, start_idx:]\n\u001b[0;32m-> 1109\u001b[0m seek_outputs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_by_batch_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeam_indices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseek_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequence_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequence_tokens, seek_outputs\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:1110\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values[batch_idx]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m   1108\u001b[0m sequence_tokens \u001b[38;5;241m=\u001b[39m seek_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, start_idx:]\n\u001b[1;32m   1109\u001b[0m seek_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m-> 1110\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_by_batch_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeam_indices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseek_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1115\u001b[0m ]\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequence_tokens, seek_outputs\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:1111\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values[batch_idx]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m   1108\u001b[0m sequence_tokens \u001b[38;5;241m=\u001b[39m seek_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, start_idx:]\n\u001b[1;32m   1109\u001b[0m seek_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1110\u001b[0m     {\n\u001b[0;32m-> 1111\u001b[0m         k: \u001b[43msplit_by_batch_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeam_indices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m seek_outputs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1113\u001b[0m     }\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1115\u001b[0m ]\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequence_tokens, seek_outputs\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:1078\u001b[0m, in \u001b[0;36mWhisperGenerationMixin._postprocess_outputs.<locals>.split_by_batch_index\u001b[0;34m(values, key, batch_idx, is_shortform, beam_indices)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [v[beam_idx]\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m (v, beam_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, beam_indices[batch_idx][: \u001b[38;5;28mlen\u001b[39m(values)])]\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 1078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(w[batch_idx][\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values)\n",
      "File \u001b[0;32m~/conda_envs/hf/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:1078\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [v[beam_idx]\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m (v, beam_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, beam_indices[batch_idx][: \u001b[38;5;28mlen\u001b[39m(values)])]\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 1078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(w[batch_idx][\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the splitting process\n",
    "split_cslu_data = create_split_cslu_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e1948-a687-40a1-8ce0-7cb4826ba7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
